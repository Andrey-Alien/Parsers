{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import artm\n",
    "\n",
    "print artm.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаю батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='',\n",
    "                                        data_format='bow_uci',\n",
    "                                        collection_name='wikistem',\n",
    "                                        target_folder='wikistem_batches',                                 \n",
    "                                        batch_size=1000)\n",
    "\n",
    "batch_vectorizer = artm.BatchVectorizer(data_path='',\n",
    "                                        data_format='bow_uci',\n",
    "                                        collection_name='wikilemm',\n",
    "                                        target_folder='wikilemm_batches',\n",
    "                                        batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаю на основе батчей словари и сохраняю их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_stem = artm.Dictionary()\n",
    "dictionary_stem.gather(data_path='wikistem_batches')\n",
    "dictionary_stem.save_text(dictionary_path='wikistem_batches/dictionary_stem.txt')\n",
    "\n",
    "dictionary_lemm = artm.Dictionary()\n",
    "dictionary_lemm.gather(data_path='wikilemm_batches')\n",
    "dictionary_lemm.save_text(dictionary_path='wikilemm_batches/fulldictionary_lemm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузка уже существующих батчей и словарей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer_stemm = artm.BatchVectorizer(data_path='wikistem_batches',\n",
    "                                        data_format='batches')\n",
    "\n",
    "dictionary_stem = artm.Dictionary()\n",
    "dictionary_stem.load_text(dictionary_path='wikistem_batches/dictionary_stem.txt')\n",
    "\n",
    "batch_vectorizer_lemm = artm.BatchVectorizer(data_path='wikilemm_batches',\n",
    "                                        data_format='batches')\n",
    "\n",
    "dictionary_lemm = artm.Dictionary()\n",
    "dictionary_lemm.load_text(dictionary_path='wikilemm_batches/dictionary_lemm.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценивание LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим саму тематическую модель, указав число тем и параметр числа прохода по документу, гиперпараметры сглаживания матриц Φ и Θ, а также используемый словарь. Кроме того, попросим сохранять матрицу Θ, чтобы потом на неё можно было взглянуть.\n",
    "\n",
    "Здесь же можно указать параметр num_processors, отвечающий за число потоков, которые будут использованы для вычислений на машине."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_lemm.filter(min_tf=2,max_tf=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_lemm.filter(max_tf=100000)\n",
    "\n",
    "lda = artm.LDA(num_topics=100, alpha=0.01, beta=0.001,\n",
    "               num_document_passes=10, dictionary=dictionary_lemm,\n",
    "               cache_theta=True,num_processors=8)\n",
    "\n",
    "lda.fit_offline(batch_vectorizer=batch_vectorizer_lemm, num_collection_passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на значения перпелексии на каждой итерации прохода по коллекции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda.perplexity_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, можно посмотреть на наиболее вероятные слова в каждой теме. Они выдаются в виде списка списков строк (каждый внутренний список соответствуюет одной теме, по порядку). Выведем их, предварительно отформатировав:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_tokens = lda.get_top_tokens(num_tokens=30)\n",
    "for i, token_list in enumerate(top_tokens):\n",
    "    print 'Topic #{0}: {1}'.format(i, token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно получить доступ к матрицам с помощью следующих вызовов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi = lda.phi_\n",
    "theta = lda.get_theta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем ещё ряд возможностей модели artm.LDA.\n",
    "\n",
    "Во-первых - это построение матрицы Θ для новых документов при наличии обученной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = artm.BatchVectorizer(data_path='kos_batches_test')\n",
    "theta_test = lda.transform(batch_vectorizer=test_batch_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, в том случае, если требуется производить различную регуляризацию каждой из тем в матрице ΦΦ, вместо скалярного значения beta можно задать список гиперпараметров, длиной в число тем, и каждая тема будет регуляризована со своим гиперпараметром:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = [0.1] * num_topics  # change as you need\n",
    "lda = artm.LDA(num_topics=15, alpha=0.01, beta=beta, num_document_passes=5, dictionary=dictionary, cache_theta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем нужную нам информацию. Вектор из пералексий, самые популярные слова и саму модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Сохраняем вектор из перплексий\n",
    "a=lda.perplexity_value\n",
    "with open('lda_lemm_100_maxtf_100000_perp.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle)\n",
    "\n",
    "\n",
    "# Сохраняем топики из слов\n",
    "top_tokens = lda.get_top_tokens(num_tokens=30)\n",
    "with open('lda_lemm_100_maxtf_100000_top_tokens.pickle', 'wb') as handle:\n",
    "    pickle.dump(top_tokens, handle)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PLSA обычная и с небольшой частичкой волшебства"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_vectorizer_lemm = artm.BatchVectorizer(data_path='wikilemm_batches',\n",
    "                                        data_format='batches')\n",
    "\n",
    "dictionary_lemm = artm.Dictionary()\n",
    "dictionary_lemm.load_text(dictionary_path='wikilemm_batches/dictionary_lemm.txt')\n",
    "\n",
    "dictionary_lemm.filter(max_tf=100000,min_tf=2)\n",
    "\n",
    "T = 800\n",
    "my_dictionary = dictionary_lemm\n",
    "batch_vectorizer = batch_vectorizer_lemm\n",
    "\n",
    "# создаём модель\n",
    "model = artm.ARTM(num_topics=T, dictionary=my_dictionary, cache_theta=False)\n",
    "# Когда в коллекции много документов, матрицу лучше не запоминать!\n",
    "\n",
    "# вешаем на неё перплексию\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity_score',\n",
    "                                      dictionary=my_dictionary))\n",
    "\n",
    "# Вешаем метрики разряженности матриц\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity_phi_score'))\n",
    "model.scores.add(artm.SparsityThetaScore(name='sparsity_theta_score'))\n",
    "\n",
    "# Вешаем запоминатель самых вероятных слов, будем запоминать 30 штук\n",
    "model.scores.add(artm.TopTokensScore(name='top_tokens_score', num_tokens=30))\n",
    "\n",
    "model.num_document_passes = 10  # сколько делать проходов по каждому документу\n",
    "\n",
    "import time\n",
    "start_time = time.time() # время работы кода \n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=10)\n",
    "print \"--- %s seconds ---\" % (time.time() - start_time)  #время работы кода\n",
    "#Юпи!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Оффлайновый алгоритм: много проходов по коллекции, один проход по документу (опционально), обновление Φ в конце каждого прохода. Используйте, если у Вас маленькая коллекция.\n",
    "\n",
    "* Онлайновый алгоритм: один проход по коллекции (опционально), много проходов по документу, обновление Φ раз в заданное количество батчей. Используйте при большой коллекции, и коллекции с быстро меняющейся тематикой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.num_document_passes = 10  # сколько делать проходов по каждому документу\n",
    "\n",
    "import time\n",
    "start_time = time.time() # время работы кода \n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=20)\n",
    "print \"--- %s seconds ---\" % (time.time() - start_time)  #время работы кода\n",
    "#Юпи!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# распечатаем все значения перплесии и других метрик\n",
    "print model.score_tracker['perplexity_score'].value      # .last_value\n",
    "print model.score_tracker['sparsity_phi_score'].value    # .last_value\n",
    "print model.score_tracker['sparsity_theta_score'].value  # .last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ежели нас что-то не устраивает, то дообучаем модель делая еще например 15 итераций.\n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Да, последний момент: метрики выгружаются из ядра при каждом обращении, поэтому для таких больших метрик, как топ-слова (или ядровые характеристики, о которых можно прочитать по данным выше ссылкам), лучше завести переменную, в которую Вы один раз всё выгрузите, а потом уже работать с ней. Поехали, просмотрим топ-слова последовательно в цикле по именам тем модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_top_tokens = model.score_tracker['top_tokens_score'].last_tokens\n",
    "\n",
    "for topic_name in model.topic_names:\n",
    "    print saved_top_tokens[topic_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Наверняка темы получились не слишком впечатляющими, да? И вот сейчас на сцену выходят они, озарённые сиянием прожекторов звёзды Тематической империи - регуляризаторы! Их задача - помочь Вам сделать модель как можно более хорошей и милой.\n",
    "\n",
    "Списки регуляризаторов и их параметров можно посмотреть здесь http://bigartm.readthedocs.org/en/master/python_interface/regularizers.html. Код работы с регуляризаторами очень похож на код работы с метриками (они всё-таки дальние родственники). Давайте в нашу модель добавим три регуляризатора: разреживание Φ, разреживание Θ и декорреляция тем. Последний в поте лица старается сделать темы как можно более различными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='sparse_phi_regularizer'))\n",
    "model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='sparse_theta_regularizer'))\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_regularizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, у Вас вызывает когнитивный диссонанс регуляризатор SmoothSparsePhi\\Theta, это он что же, и сглаживает, и разреживает? И мой ответ - именно так. Только не \"и\", а \"или\" (хотя для ΦΦ можно и \"и\", об этом дальше будет). Он может и то, и то, его действия будут зависеть от того, каким Вы зададите его коэффициент регуляризации ττ (если не знаете, что это - обращайтесь к работам архимага). ττ > 0 - будет сглаживать, ττ < 0 - разреживать. По-умолчанию все регуляризаторы получают ττ = 1.0, что, как правило, совершенно не подходит. Выбор подходящего ττ - эвристика, искусство заклинателя, порой приходится провести десятки опытов, чтобы подобрать хорошие значений. Я не буду здесь этим заниматься, просто покажу, как выставлять эти значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.regularizers['sparse_phi_regularizer'].tau = -5*1e5\n",
    "model.regularizers['sparse_theta_regularizer'].tau = -0.000000000005\n",
    "model.regularizers['decorrelator_phi_regularizer'].tau = 1e+5\n",
    "\n",
    "# В темах много общеупотребительных слов (так называемой, фоновой лексики). \n",
    "# Чтобы этого избежать, будем использовать разреживающий регуляризатор для матрицы фи.\n",
    "# Он будет подавлять слова, которые имеют большую частоту во всей коллекции.\n",
    "\n",
    "# Меняем так, чтобы добиться разряжености - более высокие значения метрик (частота нулей в матрице) \n",
    "# и интерпретируемости тем. Лучше всего применять его после того как модель сошлась чтобы добиться интерпретации!\n",
    "# То есть сначала обучили модель, после ввели метрики, после дообучили\n",
    "\n",
    "model.regularizers['sparse_phi_regularizer'].tau = -100\n",
    "model.regularizers['sparse_phi_regularizer'].tau = -5*1e4 \n",
    "\n",
    "# если вы хотите применять регуляризатор только к некоторым модальностям,\n",
    "# указывайте это в параметре class_ids: class_ids=[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выставленные значения стандартны, но при неблагоприятном стечении обстоятельств могут либо не оказать на модель существенного влияния, либо заставить её корчиться в муках. Берегите свои модели, подбирайте коэффициенты тщательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# дообучаем модель\n",
    "model.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше можно снова смотреть на метрики, поправлять коэффиценты ττ регуляризаторов и т.д.\n",
    "\n",
    "Забыл упомянуть, что регуляризаторам, как и метрикам, тоже можно объяснить, какие темы можно трогать, а какие - нельзя (они всё-таки неглупые ребята). Делается это в полной аналогии с тем, как мы проделали это для метрик. Не буду приводить пример из принципа, сами догадайтесь, это элементарно. (http://nbviewer.jupyter.org/github/bigartm/bigartm-book/blob/master/ARTM_tutorial_Fun.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Сохраняем топики из слов\n",
    "top_tokens = model.score_tracker['top_tokens_score'].last_tokens\n",
    "with open('plsa_lemm_800_maxtf_100000_mintf_2_top_tokens.pickle', 'wb') as handle:\n",
    "    pickle.dump(top_tokens, handle)   \n",
    "\n",
    "# Сохраняем метрики\n",
    "a1 = model.score_tracker['perplexity_score'].value      # .last_value\n",
    "a2 = model.score_tracker['sparsity_phi_score'].value    # .last_value\n",
    "a3 = model.score_tracker['sparsity_theta_score'].value  # .last_value\n",
    "\n",
    "a = {'perep':a1,'phi':a2,'theta':a3}\n",
    "with open('plsa_lemm_800_maxtf_100000_mintf_2_scores.pickle', 'wb') as handle:\n",
    "    pickle.dump(a, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Фильтр словарей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сейчас самое время вспомнить вид словаря, выгруженного в текстовом виде. Это были строки, каждая соответствует одному слову, и в ней 5 значений:\n",
    "\n",
    "само слово (строка), его модальность (строка), его value (число), и две загадочных величины token_tf и token_df (тоже числа).\n",
    "\n",
    "Раскроем, наконец, их смысл, который, в общем-то, довольно прост: это, соответственно, частота слова в коллекции (нормировкой это величины получается исходное value) и по-документная частота (т.е. в скольки документах коллекции слово встретилось хотя бы раз). Эти величины, как и value, генерируются библиотекой при сборе словаря. Отличие от value в том, что они не используются в регуляризаторах и метриках (пока что, по-крайней мере). Поэтому их менять бессмысленно. И даже вредно. Очень не рекомедную. Укушу.\n",
    "\n",
    "Зачем же они нужны? А для того, чтобы фильтровать словарь коллекции! Наверняка Вам не нужны слишком редкие, или же слишком частые слова. Ну или просто хочется уменьшить словарь, чтобы модель влезала в память. Или же банально на улице плохая погода, Вам стало грустно и захотелось что-то пофильтровать. Неважно. Во всех случаях решение одно, и оно очень простое - Dictionary.filter(). Посмотрите, какие у этого метода есть параметры здесь http://bigartm.readthedocs.org/en/master/python_interface/dictionary.html. Итак, будем фильтровать модальность обычных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter(min_tf=10, max_tf=2000, min_df_rate=0.01)\n",
    "\n",
    "\n",
    "dictionary.filter(max_tf=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "На всякий пожарный, или какой-либо другой случай, поясню: если суффикса _rate нет, то используется абсолютная величина, если есть - нормированная (т.е. от 0 до 1).\n",
    "\n",
    "У этого заклинания есть одна особенность - оно перезаписывает старый словарь новым. Поэтому, если не хотите потерять полноценный нефильтрованный артефакт, сохраните его сперва на диск, а потом уже с оставшейся в памяти копией творите всё, что Вам заблагорассудится (но постоянно помня про Уголовный Кодекс!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/filipp/Desktop/wikipedia basa/my_dictionary.txt',header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1488624]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(df['token'])) if df['token'][i]=='это']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        token         class_id   token_value   token_tf   token_df\n",
      "2776237   год   @default_class      0.025622  7858162.0   989101.0\n",
      "        token         class_id   token_value   token_tf   token_df\n",
      "1488624   это   @default_class      0.002081   638360.0   264293.0\n"
     ]
    }
   ],
   "source": [
    "print df.iloc[[2776237],]\n",
    "print df.iloc[[1488624],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter(max_tf=638359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "638360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## По видюхе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём объект модели и накидываем в него разных метрик!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 100   # количество тем\n",
    "model_artm = artm.ARTM(num_topics=T, topic_names=[str(i) for i in range(T)], class_ids={\"text\":1})\n",
    "# число после названия модальностей - это их веса\n",
    "\n",
    "# Перплексия\n",
    "model_artm.scores.add(artm.PerplexityScore(name='PerplexityScore',\n",
    "                                               use_unigram_document_model=False,\n",
    "                                               dictionary_name='dictionary_lemm'))\n",
    "\n",
    "# Метрики, которые отвечают за разряженность матриц\n",
    "model_artm.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore', class_id=\"text\"))\n",
    "model_artm.scores.add(artm.SparsityThetaScore(name='SparsityThetaScore'))\n",
    "\n",
    "# запоминаем самые популярные слова\n",
    "model_artm.scores.add(artm.TopTokensScore(name=\"top_words\", num_tokens=30, class_id=\"text\"))\n",
    "\n",
    "# инициализируем модель с помощью словаря \n",
    "model_artm.initialize(\"dictionary_lemm\", seed=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
