{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/cian_main_theme.png\" height=\"1000\" width=\"1000\"> \n",
    "\n",
    "<img align=\"center\" src=\"https://www.cian.ru/promo/adv/assets/images/logo.svg\" height=\"300\" width=\"300\"> \n",
    "\n",
    "\n",
    "# <center> Грабёж, разбой и другие незаконные деяния с помощью Python. <br> <br> Продолжаем ограбление. Мне бы в небо...  </center>\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Я лично качаю, но могу ускориться...  Ускоряемся! Паралелим код!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сложность по времени и сложность по памяти. \n",
    "\n",
    "Существует великая дилемма! Память или время? \n",
    "\n",
    "Многие алгоритмы предлагают выбор между объёмом памяти и скоростью. Задачу можно решить быстро, использую большой объём памяти, или медленнее, занимая меньший объём."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не так давно мы с вами написали код, который позволяет нам скачивать ЦИАН. Мы амбициозные ребята и хотели бы скачать по максимуму наблюдений. Однако, это может занять довольно много времени. Выясним сколько!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Подгружаем уже знакомые нам пакеты.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Подгружаем пакет, который умеет считать время...\n",
    "import time\n",
    "\n",
    "# А также подгружаем модуль, необходимый нам для многопоточности\n",
    "# Не забываем сначала установить его! \n",
    "from multiprocessing import Pool, cpu_count\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько времени понадобится для работы той части кода, которая вытаскивает хрефы. Сделаем это для 10 страниц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качаю хреф номер  1\n",
      "Качаю хреф номер  2\n",
      "Качаю хреф номер  3\n",
      "Качаю хреф номер  4\n",
      "Качаю хреф номер  5\n",
      "Качаю хреф номер  6\n",
      "Качаю хреф номер  7\n",
      "Качаю хреф номер  8\n",
      "Качаю хреф номер  9\n",
      "Качаю хреф номер  10\n",
      "--- 14.04449725151062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "\n",
    "### Первая часть кода. Вытаскиваем список хрефов.\n",
    "\n",
    "page_part_1 = \"https://www.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=1&room1=1&room2=1&room3=1&room4=1&room5=1&room6=1&room7=1&room9=1\"\n",
    "\n",
    "all_hrefs = [ ]  # Тут мы будем хранить наши хрефы!\n",
    "\n",
    "for i in range(1,11): # Тут регулируем количество квартир (на одной странице их 28)\n",
    "    # Делаем ссылку!\n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    \n",
    "    # Загружаем страницу\n",
    "    response = requests.get(page)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"serp-item__content__bottom__left\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)\n",
    "    print('Качаю хреф номер ',i)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На $10$ страниц ($280$ наблюдений, по $28$ квартир на каждой странице) уходит от $12$ до $15$ секунд. Чтобы скачать $150 000$ наблюдений (около $5358$ страниц) нам понадобится $15 \\cdot 535.8 = 8036$ секунд. То есть около двух с половиной часов. Это не критично и можно подождать. \n",
    "\n",
    "Посмотрим как обстоит дело с основной частью кода, которая занимается прогрузкой информации о квартирах по каждому хрефу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "скачал  10  квартир 280\n",
      "скачал  20  квартир 280\n",
      "скачал  30  квартир 280\n",
      "скачал  40  квартир 280\n",
      "скачал  50  квартир 280\n",
      "скачал  60  квартир 280\n",
      "скачал  70  квартир 280\n",
      "скачал  80  квартир 280\n",
      "скачал  90  квартир 280\n",
      "скачал  100  квартир 280\n",
      "скачал  110  квартир 280\n",
      "скачал  120  квартир 280\n",
      "скачал  130  квартир 280\n",
      "скачал  140  квартир 280\n",
      "скачал  150  квартир 280\n",
      "скачал  160  квартир 280\n",
      "скачал  170  квартир 280\n",
      "скачал  180  квартир 280\n",
      "скачал  190  квартир 280\n",
      "скачал  200  квартир 280\n",
      "скачал  210  квартир 280\n",
      "скачал  220  квартир 280\n",
      "скачал  230  квартир 280\n",
      "скачал  240  квартир 280\n",
      "скачал  250  квартир 280\n",
      "скачал  260  квартир 280\n",
      "скачал  270  квартир 280\n",
      "скачал  280  квартир 280\n",
      "--- 181.71399068832397 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "\n",
    "### Вторая часть кода. Функция, которая вытаскивает наблюдения по одной квартирке.        \n",
    "def One_Flat_Downloader(href):\n",
    "    \"\"\"\n",
    "    Функция производит выкачку по одной ЦИАНовской ссылке \n",
    "    всей существующей информации о квартире.\n",
    "    Ввод: ссылка на описание квартиры\n",
    "    Вывод: словарь с информацией о квартире    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = { }  # Задали пустой словарь, в который мы будем сохранять данные\n",
    "    \n",
    "    # Подгружаем страничку с информацией по квартире\n",
    "    response = requests.get(href)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Вытаскиваем цену на квартиру\n",
    "    price = soup.findAll('div', attrs = {'class':\"object_descr_price\"})[0].text.strip()\n",
    "    data['Цена'] = price\n",
    "    \n",
    "    # Вытаскиваем метро\n",
    "    try:\n",
    "        station = soup.findAll('p', attrs = {'class':\"objects_item_metro_prg\"})[0].a.text\n",
    "        data['Метро'] = station\n",
    "    except Exception:\n",
    "        data['Метро'] = \"NA\"\n",
    "    \n",
    "    # Вытаскиваем расстояние до метро то ли пешком то ли на машине...\n",
    "    try:\n",
    "        Do_metro = soup.findAll('p', attrs = {'class':\"objects_item_metro_prg\"})[0]\n",
    "        Do_metro = Do_metro.find('span',{'class':'object_item_metro_comment'}).text.strip()\n",
    "        data['До метро'] = Do_metro\n",
    "    except Exception:\n",
    "        data['До метро'] = \"NA\"\n",
    "    \n",
    "    # Вытаскиваем координаты квартиры\n",
    "    coordinates = soup.findAll('div',{'class':\"object-descr__map-tabs__content js-object_descr__panorama\"})[0]\n",
    "    data['Координаты'] = coordinates.panorama.attrs['point']\n",
    "    \n",
    "        \n",
    "    # Вытаскиваем количество комнат в квартире\n",
    "    komnata = soup.findAll('div', attrs = {'class':\"object_descr_title\"})[0].text.strip()\n",
    "    data['Комнаты'] = komnata\n",
    "    \n",
    "    # Вытаскиваем всё остальное\n",
    "    table = soup.findAll('table',attrs = {'class':\"object_descr_props flat sale\"})[0]\n",
    "    rows = table.find_all('tr')[1:]  # Вытащили все строки\n",
    "    for row in rows:\n",
    "        cols = [row.findAll('th')[0].text.strip(),row.findAll('td')[0].text.strip()] # очищаем каждую строку! \n",
    "        data[cols[0]] = cols[1] # запоминаем\n",
    "    return(data) \n",
    "\n",
    "\n",
    "### Третья часть кода. Собираем данные по всем хрефам и объединяем их в огромную таблицу.\n",
    "\n",
    "# В эту табличку будем собирать данные\n",
    "df = pd.DataFrame( )\n",
    "k = 0 # Это номера наблюдений\n",
    "n = len(all_hrefs)\n",
    "for item in all_hrefs:\n",
    "    k = k + 1\n",
    "    # грузим новое наблюдение\n",
    "    df1 = pd.DataFrame.from_dict(One_Flat_Downloader(item),orient='index')\n",
    "    # присваиваем этому наблюдению номер\n",
    "    df1.columns =[k]\n",
    "    # закидываем его в итоговую таблицу\n",
    "    df = df.join(df1, how='outer')\n",
    "    # Выдавать информацию о том, что сделана каждая десятая итерация! \n",
    "    if k%10 == 0:\n",
    "        print('скачал ', k, ' квартир', n)\n",
    "\n",
    "        \n",
    "df = df.T  # Для удобства транспонируем таблицу        \n",
    "\n",
    "# Добавим к итоговой таблице колонку из хрефов. Когда мы будем смотреть на данные,\n",
    "# будет возникать довольно большое количество аномалий. Хотелось бы получше изучить\n",
    "# их природу...\n",
    "\n",
    "df['Хрефы'] = all_hrefs\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выкачку $280$ наблюдений уходит около $200$ секунд. Значит для выкачки $150000$ наблюдений нам понадобится примерно $107142$ секунд ($30$ часов). Это довольна большая цифра. Такое количество времени ждать нам не очень хочется. Хотелось бы получить свои заслуженные наблюдения побыстрее. \n",
    "\n",
    "Вспомним, что у нашего компьютера есть несколько ядер и все вычисления, которые идут на нём можно распаралелить. Если бы у нас получилось качать данные в два потока, то код проработал бы $15$ часов. Если бы потоков было бы $10$, то код проработал бы $3$ часа. Если потоков было бы $30$, то код проработал бы всего один час. \n",
    "\n",
    "Обратите внимание, что мы при расчётах руководствуемся предпосылкой, от том что время работы нашего кода растёт линейно по объёму необходимой нам информации. Это довольно логично. Все циклы, прописанные нами выше, кроме самого главного никак не зависят от объема поступающей информации. Сложность нашего алгоритма по времени $O(n)$! \n",
    "\n",
    "Круто! Дело осталось за малым. Выбрать сколько потоков потянет наш компьютер и распаралелить код. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Что такое Map - Reduce и кто такой Hadoop. \n",
    "\n",
    "\n",
    "Когда мы дошли до этого момента, я загнал долгий монолог про \n",
    "[ эксперименты Google с сортировками,](https://cloud.google.com/blog/big-data/2016/02/history-of-massive-scale-sorting-experiments-at-google)  мэп-редьюс, судебные тяжбы хадупа и прочую бурду! А также показал следующие три картинки: \n",
    "\n",
    "Вот так примерно выглядит Map-Reduce:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/mapreduce_mapshuffle.png\" height=\"650\" width=\"650\"> \n",
    "\n",
    "Вот так выглядит серверная Google:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/serv_google.jpg\" height=\"800\" width=\"800\"> \n",
    "\n",
    "А вот так вот выглядит тот самый жёлтый слоник:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/FUlyankin/Parsers/master/images%20/hadoop-logo_w_500.jpg\" height=\"400\" width=\"400\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 33.66315617,  -7.2058723 ,  19.1481633 , ...,  30.70093878,\n",
       "        19.95571898,  -3.13724738])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(10,16,100000000)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_mean(x):\n",
    "    a = 0 \n",
    "    for item in x:\n",
    "        a = a + item \n",
    "    return(a/len(x))\n",
    "\n",
    "\n",
    "def Map(x):\n",
    "    return(my_mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Reduce(y):\n",
    "    z = my_mean(y)\n",
    "    return(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.99849170316\n",
      "--- 12.705657720565796 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "print(my_mean(x))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Выясним сколько ядер на нашем компьютере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае, распаралеливание кода делается по числу ядер. И обычно большее количество ядер означает большую скорость (если этот код вообще стоило паралелить).\n",
    "\n",
    "Мы будем использовать немного иную штуку, а именно многопоточную обработку. Разобьём имеющуюся у нас информацию на несколько частей и будем обрабатывать её в несколько потоков. Есть смысл поэкспериментировать с количеством потоков. Если их будет очень много, то код будет тратить лишнее время на переключение между ними. Именно так это происходит ниже... Сейчас мы распаралелим код так, что он будет работать дольше, чем обычный...\n",
    "\n",
    "Итак, создаём 30 воркеров и пробуем прогнать наш алгоритм.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Делим весь список из имён на 10 частей\n",
    "division = 100\n",
    "parts = [round(len(x)/division)*i for i in range(division)]\n",
    "parts.append(len(x))\n",
    "names = [x[parts[i]:parts[i+1]] for i in range(division)]\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool = ThreadPool(division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 16.799778938293457 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "\n",
    "results = pool.map(Map, names)\n",
    "Reduce(results)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Применяем концепцию Map — Reduce к нашему случаю. \n",
    "\n",
    "Итак, наш код состоит из двух частей. Первая — тащит хрефы и работает около 2-х часов. Её особо трогать не хочется. Распаралеливание может принести нам много головной боли. Нужно выбрать с какой страницы стартует какая часть кода и на какой странице какая часть кода свою работу заканчивает. Забьём на этот кусок и просто подождём пару часов. Два часа это не двое суток! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "\n",
    "### Первая часть кода. Вытаскиваем список хрефов.\n",
    "\n",
    "page_part_1 = \"https://www.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=1&room1=1&room2=1&room3=1&room4=1&room5=1&room6=1&room7=1&room9=1\"\n",
    "\n",
    "all_hrefs = [ ]  # Тут мы будем хранить наши хрефы!\n",
    "\n",
    "for i in range(1,5361): # Тут регулируем количество квартир (на одной странице их 28)\n",
    "    # Делаем ссылку!\n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    \n",
    "    # Загружаем страницу\n",
    "    response = requests.get(page)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"serp-item__content__bottom__left\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)\n",
    "    print('Качаю хреф номер ',i)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На всякий случай сохраним вектор из ссылок. Чтобы не потерять структуру данных (вектор), запишем их в формате `pickle`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_refs.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_hrefs, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('all_refs.pickle', 'rb') as f:\n",
    "     all_hrefs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы изначально и планировали, код проработал около 2,5 часов. Круто! Займёмся распаралеливанием второго куска кода. Функция `One_Flat_downloader` была написана нами довольно удачно. Она выкачивает только одно наблюдение и никак не повязана на циклы. Вообще не будем трогать её. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Вторая часть кода. Функция, которая вытаскивает наблюдения по одной квартирке.    \n",
    "    \n",
    "def One_Flat_Downloader(href):\n",
    "    \"\"\"\n",
    "    Функция производит выкачку по одной ЦИАНовской ссылке \n",
    "    всей существующей информации о квартире.\n",
    "    Ввод: ссылка на описание квартиры\n",
    "    Вывод: словарь с информацией о квартире    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = { }  # Задали пустой словарь, в который мы будем сохранять данные\n",
    "    \n",
    "    # Подгружаем страничку с информацией по квартире\n",
    "    response = requests.get(href)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Вытаскиваем цену на квартиру\n",
    "    price = soup.findAll('div', attrs = {'class':\"object_descr_price\"})[0].text.strip()\n",
    "    data['Цена'] = price\n",
    "    \n",
    "    # Вытаскиваем метро\n",
    "    try:\n",
    "        station = soup.findAll('p', attrs = {'class':\"objects_item_metro_prg\"})[0].a.text\n",
    "        data['Метро'] = station\n",
    "    except Exception:\n",
    "        data['Метро'] = \"NA\"\n",
    "    \n",
    "    # Вытаскиваем расстояние до метро то ли пешком то ли на машине...\n",
    "    try:\n",
    "        Do_metro = soup.findAll('p', attrs = {'class':\"objects_item_metro_prg\"})[0]\n",
    "        Do_metro = Do_metro.find('span',{'class':'object_item_metro_comment'}).text.strip()\n",
    "        data['До метро'] = Do_metro\n",
    "    except Exception:\n",
    "        data['До метро'] = \"NA\"\n",
    "    \n",
    "    # Вытаскиваем количество комнат в квартире\n",
    "    komnata = soup.findAll('div', attrs = {'class':\"object_descr_title\"})[0].text.strip()\n",
    "    data['Комнаты'] = komnata\n",
    "    \n",
    "    # Вытаскиваем координаты квартиры\n",
    "    coordinates = soup.findAll('div',{'class':\"object-descr__map-tabs__content js-object_descr__panorama\"})[0]\n",
    "    data['Координаты'] = coordinates.panorama.attrs['point']\n",
    "    \n",
    "    # Вытаскиваем всё остальное\n",
    "    table = soup.findAll('table',attrs = {'class':\"object_descr_props flat sale\"})[0]\n",
    "    rows = table.find_all('tr')[1:]  # Вытащили все строки\n",
    "    for row in rows:\n",
    "        cols = [row.findAll('th')[0].text.strip(),row.findAll('td')[0].text.strip()] # очищаем каждую строку! \n",
    "        data[cols[0]] = cols[1] # запоминаем\n",
    "    return(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Третья часть нашего кода — главный цикл, который и выкачивает информацию. Как раз в ней и происходит скачёк времени при росте объёма входа. Превратим этот цикл в функции `Map` и `Reduce`. \n",
    "\n",
    "Функция `Map` будет обращаться к вектору из ссылок `part_hrefs` и вытаскивать по ним всю информацию. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Map(part_hrefs):\n",
    "    # В эту табличку будем собирать данные\n",
    "    df = pd.DataFrame( )\n",
    "    k = 0 # Это номера наблюдений\n",
    "    n = len(part_hrefs)\n",
    "    for item in part_hrefs:\n",
    "        k = k + 1\n",
    "        # грузим новое наблюдение/\n",
    "        df1 = pd.DataFrame.from_dict(One_Flat_Downloader(item),orient='index')\n",
    "        # присваиваем этому наблюдению номер\n",
    "        df1.columns =[k]\n",
    "        # закидываем его в итоговую таблицу\n",
    "        df = df.join(df1, how='outer')\n",
    "        # Выдавать информацию о том, что сделана каждая десятая итерация! \n",
    "        if k%100 == 0:\n",
    "            print('сделано ', k, ' итераций из', n)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаём потоки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Раздробим весь список из ссылок на какое-то количество частей. Например, на 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_hrefs = all_hrefs[:60000]\n",
    "len(current_hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Делим весь список из имён на 30 частей\n",
    "parts = [round(len(current_hrefs)/30)*i for i in range(30)]\n",
    "parts.append(len(current_hrefs))\n",
    "names = [current_hrefs[parts[i]:parts[i+1]] for i in range(30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, работает ли функция `Map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Map(names[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем по каждому из 30 векторов со ссылками информацию. Все потоки работаю независимо друг от друга и записывают информацию в свои таблички. После все таблички закидываются в вектор `l`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "    \n",
    "pool = ThreadPool(30)\n",
    "l = pool.map(Map, names)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые 60 000 наблюдений скачались за 8544 секунд (2 с лишним часа). Возможно, мы  переборщили с количеством потоков. Ожидалось, что алгоритм проработает около часа. При прямой работе алгоритм бы работал около 12 часов. Можно провести парочку экспериментов, но в любом случае получилось быстрее, чем 12 часов! Ну и господь с этой шайтан-машиной..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остался последний шаг — `Reduce` - шаг. Функция `Reduce` будет брать таблички из списка `l` и объединять их все в один большой `dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Reduce(l):\n",
    "    df = pd.DataFrame( )  # новый датафрейм, куда мы всё закинем! \n",
    "    k = 0\n",
    "    for item in l:        # Этот цикл уже вам знаком...\n",
    "        n = item.shape[1] + k\n",
    "        item.columns = range(k,n)\n",
    "        df = df.join(item, how = 'outer')\n",
    "        k = n \n",
    "    return(df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time() # время работы кода \n",
    "\n",
    "itog_data = Reduce(l) \n",
    "itog_data = itog_data.T             # Для удобства транспонируем таблицу        \n",
    "itog_data['Хрефы'] = current_hrefs  # Добавим к итоговой таблице колонку из хрефов.\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))  #время работы кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Смотрим на итоговые характеристики нашей таблицы и сохраняем результат на свой компьютер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itog_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(itog_data,'CIAN_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
