{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5357.142857142857"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150000/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "### Первая часть кода. Вытаскиваем список хрефов.\n",
    "\n",
    "page_part_1 = \"https://www.cian.ru/cat.php?deal_type=sale&engine_version=2&offer_type=flat&p=\"\n",
    "page_part_2 = \"&region=1&room1=1&room2=1&room3=1&room4=1&room5=1&room6=1&room7=1&room9=1\"\n",
    "\n",
    "all_hrefs = [ ]  # Тут мы будем хранить наши хрефы!\n",
    "\n",
    "for i in range(1,11): # Тут регулируем количество квартир (на одной странице их 28)\n",
    "    # Делаем ссылку!\n",
    "    page = page_part_1 + str(i) + page_part_2\n",
    "    \n",
    "    # Загружаем страницу\n",
    "    response = requests.get(page)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Забираем себе хрефы и очищаем их\n",
    "    hrefs = soup.findAll('div', attrs = {'class':\"serp-item__content__bottom__left\"})\n",
    "    clean_hrefs = [item.a.attrs['href'] for item in hrefs]\n",
    "    all_hrefs.extend(clean_hrefs)\n",
    "    print('Качаю хреф номер ',i)\n",
    "    \n",
    "\n",
    "### Вторая часть кода. Функция, которая вытаскивает наблюдения по одной квартирке.    \n",
    "    \n",
    "def One_Flat_Downloader(href):\n",
    "    \"\"\"\n",
    "    Функция производит выкачку по одной ЦИАНовской ссылке \n",
    "    всей существующей информации о квартире.\n",
    "    Ввод: ссылка на описание квартиры\n",
    "    Вывод: словарь с информацией о квартире    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = { }  # Задали пустой словарь, в который мы будем сохранять данные\n",
    "    \n",
    "    # Подгружаем страничку с информацией по квартире\n",
    "    response = requests.get(href)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    \n",
    "    # Вытаскиваем цену на квартиру\n",
    "    price = soup.findAll('div', attrs = {'class':\"object_descr_price\"})[0].text.strip()\n",
    "    data['Цена'] = price\n",
    "    \n",
    "    # Вытаскиваем метро\n",
    "    try:\n",
    "        station = soup.findAll('p', attrs = {'class':\"objects_item_metro_prg\"})[0].a.text\n",
    "        data['Метро'] = station\n",
    "    except Exception:\n",
    "        data['Метро'] = \"NA\"\n",
    "    \n",
    "    # Вытаскиваем расстояние до метро то ли пешком то ли на машине...\n",
    "    try:\n",
    "        Do_metro = soup.findAll('p', attrs = {'class':\"objects_item_metro_prg\"})[0]\n",
    "        Do_metro = Do_metro.find('span',{'class':'object_item_metro_comment'}).text.strip()\n",
    "        data['До метро'] = Do_metro\n",
    "    except Exception:\n",
    "        data['До метро'] = \"NA\"\n",
    "    \n",
    "    # Вытаскиваем координаты квартиры\n",
    "    coordinates = soup.findAll('div',{'class':\"object-descr__map-tabs__content js-object_descr__panorama\"})[0]\n",
    "    data['Координаты'] = coordinates.panorama.attrs['point']\n",
    "    \n",
    "        \n",
    "    # Вытаскиваем количество комнат в квартире\n",
    "    komnata = soup.findAll('div', attrs = {'class':\"object_descr_title\"})[0].text.strip()\n",
    "    data['Комнаты'] = komnata\n",
    "    \n",
    "    # Вытаскиваем всё остальное\n",
    "    table = soup.findAll('table',attrs = {'class':\"object_descr_props flat sale\"})[0]\n",
    "    rows = table.find_all('tr')[1:]  # Вытащили все строки\n",
    "    for row in rows:\n",
    "        cols = [row.findAll('th')[0].text.strip(),row.findAll('td')[0].text.strip()] # очищаем каждую строку! \n",
    "        data[cols[0]] = cols[1] # запоминаем\n",
    "    return(data) \n",
    "\n",
    "\n",
    "### Третья часть кода. Собираем данные по всем хрефам и объединяем их в огромную таблицу.\n",
    "\n",
    "# В эту табличку будем собирать данные\n",
    "df = pd.DataFrame( )\n",
    "k = 0 # Это номера наблюдений\n",
    "n = len(all_hrefs)\n",
    "for item in all_hrefs:\n",
    "    k = k + 1\n",
    "    # грузим новое наблюдение\n",
    "    df1 = pd.DataFrame.from_dict(One_Flat_Downloader(item),orient='index')\n",
    "    # присваиваем этому наблюдению номер\n",
    "    df1.columns =[k]\n",
    "    # закидываем его в итоговую таблицу\n",
    "    df = df.join(df1, how='outer')\n",
    "    # Выдавать информацию о том, что сделана каждая десятая итерация! \n",
    "    if k%10 == 0:\n",
    "        print('скачал ', k, ' квартир', n)\n",
    "\n",
    "        \n",
    "df = df.T  # Для удобства транспонируем таблицу        \n",
    "\n",
    "# Добавим к итоговой таблице колонку из хрефов. Когда мы будем смотреть на данные,\n",
    "# будет возникать довольно большое количество аномалий. Хотелось бы получше изучить\n",
    "# их природу...\n",
    "\n",
    "df['Хрефы'] = all_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(df,'CIAN_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
