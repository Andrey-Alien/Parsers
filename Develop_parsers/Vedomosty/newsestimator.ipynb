{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка выгруженных статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def monthlength(month,year):\n",
    "    if year % 4 == 0:\n",
    "         VisYear = 29\n",
    "    else:\n",
    "         VisYear = 28\n",
    "    return [31,VisYear,31,30,31,30,31,31,30,31,30,31][month]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, подсчитывающая частоту каждого слова в статье и выдающая словарь {слово : частота}, а после в DataFrame. Соберем все статьи за конкретный день в одну статью, так как нам необходимо оценить направленность политики в течение дня, и будем работать с суперстатьями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Freq(text):\n",
    "    text = text.replace(\",\",'').replace(\".\",'').replace(\"-\",\" \")\n",
    "    tx_new = text.split()\n",
    "    frequencies = {}\n",
    "    for item in tx_new:\n",
    "        if item in frequencies:\n",
    "            frequencies[item][0]+=1\n",
    "        else:\n",
    "            frequencies[item]=[1]\n",
    "    return(frequencies)\n",
    "\n",
    "def DF(text):\n",
    "    #Преообразуем выборку в DataFrame \n",
    "    df = pd.DataFrame(Freq(text))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, делающая DataFrame по одному дню какого-то месяца какого-то года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DayFrame(day,mouth,year):\n",
    "    way = \"C:/Users/zero/Desktop/ved_dat/\" + str(year) + \"/\" + str(mouth) + \".\" +str(year) + \"/\"\n",
    "    hh = pickle.load(open(way + str(day)+\".txt\", \"rb\"))\n",
    "    text = [hh[i]['text'] for i in range(len(hh))]\n",
    "    daytext = ''\n",
    "    for k in text:\n",
    "        daytext = daytext + \" \" + k\n",
    "    initial = DF(daytext)\n",
    "    return(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame за месяц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MouthFrame(mouth,year):\n",
    "    way = \"C:/Users/zero/Desktop/ved_dat/\"+ str(year) + \"/\" + str(mouth) + \".\" + str(year) + \"/\"\n",
    "    tt = [len(pickle.load(open(way + str(hh)+\".txt\", \"rb\"))) for hh in range(1,monthlength(mouth-1,year)+1)]\n",
    "    exist = [i for i in range(len(tt)) if tt[i]>0]\n",
    "    k = DayFrame(exist[0]+1,mouth,year)\n",
    "    for m in exist:\n",
    "        k = k.append(DayFrame(m+1,mouth,year))    \n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "begin = MouthFrame(1,2015)\n",
    "for item in range(2,13):\n",
    "    begin=begin.append(MouthFrame(item,2015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В полученном выше DataFrame есть несколько проблем, с каждой из которых мы ниже будем бороться.\n",
    "\n",
    "1) Полиморфизм. Каждое слово имеет кучу различных падежей, чисел, склонений. Такие случаи надо как-то унифицировать. Существует два способа бороться с этой проблемой: стэминг (Stemming) и Лемматизация (Lemmatization). Стэминг подразумевает отсечение всех приставок, окончаний и суффиксов. Главный недостаток этого способа в том, что он никогда не переведет слово люди в слово человек. Лемматизация подразумевает использование заранее созданного словаря для перевода всех слов в нормальную форму. Мы для решения этой проблемы будем использовать лематизацию, реализованную для русского языка в пакете pymorthy2.\n",
    "\n",
    "2) Стоп слова. Зашкаливающее количество предлогов в каждой статье. Можно искусственно вести лист из стоп слов и отскать их с помощью этого листа. Однако мы поступим иначе и просто отсеем слова, встречающиеся более чем в 90-95% статей.\n",
    "\n",
    "3) Редкие слова. Из-за огромного коилчества редких слов, имеющаяся у нас матрица разряжена и для оценки 10 с лишним тысяч коэффициентов имеется лишь около 1000 наблюдений. Придется выкидывать редкие слова для того, чтобы этот дисбаланс преодалеть.\n",
    "\n",
    "4) Орфографические ошибки в статьях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения первой проблемы вошьем методы pymorphy2 в функции, написанные выше и получим новый DataFrame.\n",
    "\n",
    "Установим пакет через pip, вбив соответствующую команду в командную строку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим pymorthy2 в нашу функцию и заново задатафреймим статейки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Freq(text):\n",
    "    text = text.replace(\",\",'').replace(\".\",'').replace(\"-\",\" \")\n",
    "    tx_new = text.split()\n",
    "    tx_pm = [morph.parse(item)[0].normal_form for item in tx_new]     \n",
    "    frequencies = {}\n",
    "    for item in tx_pm:\n",
    "        if item in frequencies:\n",
    "            frequencies[item][0]+=1\n",
    "        else:\n",
    "            frequencies[item]=[1]\n",
    "    return(frequencies)\n",
    "\n",
    "def DF(text):\n",
    "    #Преообразуем выборку в DataFrame \n",
    "    df = pd.DataFrame(Freq(text))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmdf2015 = MouthFrame(1,2015)\n",
    "for item in range(2,13):\n",
    "    pmdf2015=pmdf2015.append(MouthFrame(item,2015))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично соберём DataFrame за 2014, январь - апрель 2016 года и июль - декабрь 2013 года. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmdf2014 = MouthFrame(1,2014)\n",
    "for item in range(2,13):\n",
    "    pmdf2014=pmdf2014.append(MouthFrame(item,2014))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmdf2013 = MouthFrame(7,2013)\n",
    "for item in range(8,13):\n",
    "    pmdf2013=pmdf2013.append(MouthFrame(item,2013))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pmdf2016 = MouthFrame(1,2016)\n",
    "for item in range(2,5):\n",
    "    pmdf2016=pmdf2016.append(MouthFrame(item,2016))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы собрали четыре DataFrame. Объединим их и приведем в порядок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigdf = pmdf2013\n",
    "bigdf = bigdf.append(pmdf2014)\n",
    "bigdf = bigdf.append(pmdf2015)\n",
    "bigdf = bigdf.append(pmdf2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего имеем в рассмотрении 706 дней, 15492 слова. При этом классифицированы из них 266. Очистим DataFrame от стоп слов и редких слов так, чтобы наблюдений хватило для оценки коэффициентов в логистической регрессии. Заменим все NaN в DataFrame на нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigdf.fillna(0, inplace=True)\n",
    "dim = bigdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем насколько часто встречаются в нашей выборке отдельные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def WordCount(df,column):\n",
    "    name = df.columns\n",
    "    r = np.array(df[name[column]])\n",
    "    k = 0\n",
    "    for item in r:\n",
    "        if item !=0:\n",
    "            k=k+1\n",
    "    return(k/dim[0]*100)    \n",
    "\n",
    "cowo = np.array([WordCount(bigdf,jtem) for jtem in range(dim[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['мочь', 'от', 'тот', 'для', 'регулятор', 'а', 'рубль', 'до', 'который',\n",
      "       'за', 'из', 'к', 'он', 'о', 'банк', 'что', 'банка', 'не', 'россия',\n",
      "       'это', 'год', 'быть', 'по', 'с', 'на', 'цб', 'и', 'в', 'определение',\n",
      "       'приватный', 'просмотр', 'режим'],\n",
      "      dtype='object')\n",
      "[  81.72804533   82.57790368   83.28611898   83.42776204   84.70254958\n",
      "   85.12747875   85.83569405   87.11048159   87.81869688   88.52691218\n",
      "   88.52691218   89.80169972   89.80169972   90.79320113   90.79320113\n",
      "   91.78470255   91.92634561   92.63456091   93.20113314   93.62606232\n",
      "   94.0509915    94.47592068   96.31728045   97.02549575   99.00849858\n",
      "   99.15014164   99.15014164   99.71671388  100.          100.          100.\n",
      "  100.        ]\n"
     ]
    }
   ],
   "source": [
    "name = bigdf.columns\n",
    "sortname = np.argsort(cowo)\n",
    "print(name[sortname[-32:]])\n",
    "print(cowo[sortname[-32:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на самые редкие слова. Всего в одну статью. попадает 6053 слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['навнутренний', 'натиксис', 'натакий', 'насыщенный', 'насыпь',\n",
      "       'насущный', 'настроенность', 'финаму', 'настраивать', 'настоятельно',\n",
      "       'настороженность', 'настороженно', 'настепень', 'насовсем', 'наслучай',\n",
      "       'наследница', 'наследить', 'наследие', 'финансито', 'финарс'],\n",
      "      dtype='object')\n",
      "[ 0.14164306  0.14164306  0.14164306  0.14164306  0.14164306  0.14164306\n",
      "  0.14164306  0.14164306  0.14164306  0.14164306  0.14164306  0.14164306\n",
      "  0.14164306  0.14164306  0.14164306  0.14164306  0.14164306  0.14164306\n",
      "  0.14164306  0.14164306]\n",
      "[6053, 2155, 1085, 728, 481, 398, 320, 278, 219, 174, 183, 128, 119, 122, 99, 94, 95, 84, 80, 83, 63, 53, 70, 57, 60, 50, 52, 50, 50]\n"
     ]
    }
   ],
   "source": [
    "print(name[sortname[:20]])\n",
    "print(cowo[sortname[:20]])\n",
    "\n",
    "def WF(how):\n",
    "    l=0\n",
    "    for item in cowo:\n",
    "        if item == how/dim[0]*100:\n",
    "            l=l+1\n",
    "    return(l)\n",
    "\n",
    "print([WF(jtem) for jtem in range(1,30)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слов с частотой менее 1% всего будет 11220 штук. С частотой менее 0.5% 9293 слова. Нам необходимо получить количество коэффициентов, которое не привысило бы количество наблюдений, то есть было бы меньше чем 266. Чтобы получить ровно 266 коэффициентов нужно выкинуть 15226 слова. Если отсеять слова, встречающиеся менее чем в 5% статей, то будет выброшено 13716 слов. Необходимо будет оценить 757 коэффициентов по 958 наблюдениям. Остановимся пока что на этой цифре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15316"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "for item in cowo:\n",
    "    if item < 40:\n",
    "        l=l+1\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбрасываем из DataFrame редкие слова. Их выброс заодно решит проблему орфографических ошибок. Также выбрасываем слова, встречающиеся чаще, чем в 70% статей. Отметим, что слово а встречается с частотой 53%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigdf = bigdf.drop(name[sortname[:14736]], axis=1)\n",
    "bigdf.drop(name[sortname[-19:]],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим тренировочную и тестовую выборкии. В тренировочную выборку войдёт 2015 год, в тестовую 2016 год."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim3 = pmdf2013.shape\n",
    "dim4 = pmdf2014.shape\n",
    "dim5 = pmdf2015.shape\n",
    "dim6 = pmdf2016.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train0 = dim3[0] + dim4[0]\n",
    "train1 = dim[0] - dim6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(np.array(bigdf)[train0:train1])\n",
    "test = pd.DataFrame(np.array(bigdf)[train1:dim[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myaw = bigdf.columns\n",
    "train.columns = myaw\n",
    "test.columns = myaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим ответы для статей из тренировочной выбоки и ответы для статей из тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "958"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Приятные мелочи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рисунок с гистограммой для презентации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(cowo[sortname[14700:15492]], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация по заголовкам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DayFrame(day,mouth,year):\n",
    "    way = \"C:/Users/zero/Desktop/ved_dat/\" + str(year) + \"/\" + str(mouth) + \".\" +str(year) + \"/\"\n",
    "    hh = pickle.load(open(way + str(day)+\".txt\", \"rb\"))\n",
    "    text = [hh[i]['title'] for i in range(len(hh))]\n",
    "    daytext = ''\n",
    "    for k in text:\n",
    "        daytext = daytext + \" \" + k\n",
    "    initial = DF(daytext)\n",
    "    return(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmdf2015 = MouthFrame(1,2015)\n",
    "for item in range(2,13):\n",
    "    pmdf2015=pmdf2015.append(MouthFrame(item,2015))\n",
    "    \n",
    "pmdf2014 = MouthFrame(1,2014)\n",
    "for item in range(2,13):\n",
    "    pmdf2014=pmdf2014.append(MouthFrame(item,2014))\n",
    "\n",
    "pmdf2013 = MouthFrame(7,2013)\n",
    "for item in range(8,13):\n",
    "    pmdf2013=pmdf2013.append(MouthFrame(item,2013))\n",
    "    \n",
    "pmdf2016 = MouthFrame(1,2016)\n",
    "for item in range(2,5):\n",
    "    pmdf2016=pmdf2016.append(MouthFrame(item,2016))\n",
    "    \n",
    "bigdf = pmdf2013\n",
    "bigdf = bigdf.append(pmdf2014)\n",
    "bigdf = bigdf.append(pmdf2015)\n",
    "bigdf = bigdf.append(pmdf2016)\n",
    "\n",
    "bigdf.fillna(0, inplace=True)\n",
    "dim = bigdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Частые слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['коридор', 'кредит', 'набиуллин', 'рынок', 'из', 'к', 'от', 'мочь', 'о',\n",
      "       'курс', 'до', 'валютный', 'для', 'отозвать', 'миллиард', 'за', 'у',\n",
      "       'ставка', 'не', 'лицензия', 'по', 'с', 'год', 'и', 'центробанк',\n",
      "       'россия', 'рубль', 'банка', 'банк', 'на', 'в', 'цб'],\n",
      "      dtype='object')\n",
      "[ 11.17647059  11.32352941  12.05882353  12.35294118  12.5         13.08823529\n",
      "  13.08823529  14.41176471  15.          15.          15.58823529\n",
      "  15.58823529  15.58823529  16.91176471  19.55882353  20.44117647\n",
      "  21.17647059  22.05882353  23.67647059  23.82352941  24.26470588\n",
      "  25.14705882  25.29411765  25.88235294  26.32352941  27.5         31.32352941\n",
      "  32.20588235  35.58823529  45.          56.17647059  94.55882353]\n"
     ]
    }
   ],
   "source": [
    "def WordCount(df,column):\n",
    "    name = df.columns\n",
    "    r = np.array(df[name[column]])\n",
    "    k = 0\n",
    "    for item in r:\n",
    "        if item !=0:\n",
    "            k=k+1\n",
    "    return(k/dim[0]*100)    \n",
    "\n",
    "cowo = np.array([WordCount(bigdf,jtem) for jtem in range(dim[1])])\n",
    "\n",
    "name = bigdf.columns\n",
    "sortname = np.argsort(cowo)\n",
    "print(name[sortname[-32:]])\n",
    "print(cowo[sortname[-32:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Редкие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['протест', 'ловушка', 'подконтрольный', 'стагфляционный', 'лишение',\n",
      "       'лишать', 'личный', 'простой', 'подмосковный', 'поднадзорный',\n",
      "       'листинг', 'лист', 'линк', 'лимитировать', 'сталин', 'поднимать',\n",
      "       'стандартный', 'ремонт', 'логотип', 'поделиться'],\n",
      "      dtype='object')\n",
      "[ 0.14705882  0.14705882  0.14705882  0.14705882  0.14705882  0.14705882\n",
      "  0.14705882  0.14705882  0.14705882  0.14705882  0.14705882  0.14705882\n",
      "  0.14705882  0.14705882  0.14705882  0.14705882  0.14705882  0.14705882\n",
      "  0.14705882  0.14705882]\n",
      "[1352, 484, 222, 135, 96, 66, 55, 49, 29, 26, 25, 36, 21, 16, 9, 10, 6, 21, 8, 13, 10, 7, 6, 5, 6, 9, 5, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "print(name[sortname[:20]])\n",
    "print(cowo[sortname[:20]])\n",
    "\n",
    "def WF(how):\n",
    "    l=0\n",
    "    for item in cowo:\n",
    "        if item == how/dim[0]*100:\n",
    "            l=l+1\n",
    "    return(l)\n",
    "\n",
    "print([WF(jtem) for jtem in range(1,30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2596"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "for item in cowo:\n",
    "    if item < 2:\n",
    "        l=l+1\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего 2838 коэффициентов. Наблюдений всего 266. Отсечем слова, встречающиеся менее чем в 2% статей и более чем в 50% статей. После редукции требуется оценить 191 коэффициент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(680, 157)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigdf = bigdf.drop(name[sortname[:2645]], axis=1)\n",
    "bigdf.drop(name[sortname[-2:]],axis=1,inplace=True)\n",
    "bigdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим тренировочную и тестовую выборкии. В тренировочную выборку войдёт 2015 год, в тестовую 2016 год."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim3 = pmdf2013.shape\n",
    "dim4 = pmdf2014.shape\n",
    "dim5 = pmdf2015.shape\n",
    "dim6 = pmdf2016.shape\n",
    "\n",
    "train0 = dim3[0] + dim4[0]\n",
    "train1 = dim[0] - dim6[0]\n",
    "\n",
    "train = pd.DataFrame(np.array(bigdf)[train0:train1])\n",
    "test = pd.DataFrame(np.array(bigdf)[train1:dim[0]])\n",
    "\n",
    "myaw = bigdf.columns\n",
    "train.columns = myaw\n",
    "test.columns = myaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим ответы для статей из тренировочной выбоки и ответы для статей из тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 157)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 30]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mouth=9\n",
    "way = \"C:/Users/zero/Desktop/ved_dat/\"+ str(2015) + \"/\" + str(mouth) + \".\" + str(2015) + \"/\"\n",
    "tt = [len(pickle.load(open(way + str(hh)+\".txt\", \"rb\"))) for hh in range(1,monthlength(mouth-1,2015)+1)]\n",
    "exist = [i+1 for i in range(len(tt)) if tt[i]>0]\n",
    "exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
